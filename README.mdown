# Norm
Norm is a MarkLogic Denormalisation helper library. A Denormalisation in MarkLogic allows you to alter the structure of multiple incoming documents and create composite documents 
to act as the results of a search application. Scenarios supported include:-

- Creating a doc with a different structure or subset of elements from a source document, perhaps including static content
 - Changing structure - Adding parent elements, renaming, changing namespace
 - Static content - E.g. &lt;generatedby&gt;Adam's denorm&lt;/generatedby&gt;
- Shredding one incoming document in to several documents
 - A single CSV file added to MarkLogic could become many documents, one per row
 - Also useful for creating documents to act as rows for ODBC views, where a tree structure exists that would otherwise produce false positive results by index resolution
- Combining multiple related documents in to a single aggregate document
 - E.g. for merging thin Relational documents for an Order, Order items, delivery address in to a single OrderDetails document
 - Combining Episode, Series, Brand, and Scheduling documents in to several Programme-Availability documents
- In MarkLogic 7, generate an inferred triple graph document based on multiple related triples.
 - E.g. Adam's father is Cliff, Cliffs father is Harry, therefore create a triple for Adam's grandfather is Harry
 - Currently limited to source documents containing one triple each (May be able to workaround this using the shredding feature with foreign key relationship to same document)

## Design
Norm is configuration driven. This configuration is saved in multiple files, one per denormalisation target document, in the norm-config collection. 
A single library of currently 629 lines of XQuery called lib-norm.xqy is responsible for carrying out the denormalisation work. This library also
performs sanity checks to find any enabled denormalisations matching the given source document URI. If none match, no denormalisation is carried out.

Norm also checks that all required content is present before generating a denormalisation. This is useful where you do not want an incomplete denormalisation
in the database. Sources can be set to optional if a partial denormalisation is a valid output.

Norm works by defining one or more sources. These are document collections that the content comes from, along with primary key definitions. Foreign key definitions
are supported. Norm works out the relationships between all sources, and which source matches the provided newly added document, in order to locate related content
in the repository that should be included in the output denormalisation. This uses element values, element attribute values and xdmp:unpath to find primary key values.
Source content is located using element range query, element attribute range query and path range query, as required. A list of required indexes and their configuration
can be given for a specified denormalisation configuration using the n:list-indexes-required($docuri as xs:string) function.

Once related content has been found one or more output documents are generated using the provided template, uri pattern and collection list. Template elements and attributes
must reference content from the sources, optionally restricting what is extracted using XPath. It is also possible to provide hard coded values and XML using the n:static element.
URIs for output content can be specified using wildcards like ##s1:pk# for the primary key string value from source with id s1, or ##s1:pk:1## for the first primary key (if
multiple are specified). ##s1:uri## will match the URI of the related source document. ##auto## will provide a random long number as a string in the URI.

It should be noted that because multiple sources can be related to each other, and there may be several 1:many relationships, a single new/updated document may result in
multiple denormalisation documents being created. These can sometimes be a large number. In this case you may want to run the triggers as post commit and live with the
eventually-consistent database for the denormalised content.

## Installation
Currently there is no UI. To install to a target content database:-

- Copy /src/app/models/lib-norm.xqy to your modules database at /app/models/lib-norm.xqy
- Add the trigger /src/app/models/trigger-norm-create.xqy at the same folder location in the modules database
- Enabled this trigger on one or more incoming document collections, or globally (there are internal checks to ensure it works globally)
- Add a denormalisation configuration to the 'norm-config' collection. See files in the samples folder for examples
- Ingest your content and watch the magic happen! (Also check the error logs in case you got it wrong the first time!)

If you just want to test Norm from QConsole against a test database then you can instead use the embedded Roxy app to deploy a new DB and app server:-

- Edit /src/app/config/config.xqy and check the username, password, and port numbers in use. Edit as necessary.
- From a terminal in the main norm directory execute ./ml local bootstrap followed by ./ml local deploy modules
- From a QConsole attached to the new norm-content database, open the QConsole file in norm.xml in the main norm directory. This shows working examples and useful scripts.
- Load in sample content using the scripts in data/*.xml (These are actually XQuery files that load the data with the correct collections and URIs) 
- Open the 'Run denormalisation' query in QConsole, alter the Doc URI to the one to simulate a trigger firing on, and execute the script 

## Requirements
* MarkLogic 6.0 or above (lower if not using path range indexes)
* [Ruby](http://www.ruby-lang.org/en/) - Required for Roxy Deployer only.
* [Java (jdk)](http://www.oracle.com/technetwork/java/javase/downloads/index.html) - Only if you wish to run the Roxy Deployer [XQSync](http://developer.marklogic.com/code/xqsync, XQSync) or [RecordLoader](http://developer.marklogic.com/code/recordloader) commands.

## Getting Help
Email me at adam.fowler@marklogic.com also read my blog at (http://adamfowlerml.wordpress.com)

## Common gotchas
In order to execute successfully the code requires particular indexes. Once you've created a denormalisation configuration execute the list indexes function for your configuration. 
An example of this is in the norm.xml QConsole file. Load this in to QConsole via Import Workspace to see a sample.

## Performance
The code is constantly profiled to find any performance issues, and the internal content generation functions are tweaked to avoid these issues. If you notice a
particular problem please fill out an issue on this GitHub project, including sample data and configuration where possible.

On an old MacBook Pro 17" from 2008 a joining denormalisation (2 new documents generated) can be executed in 0.0108 seconds. This is with minimal logging enabled.

I have since tested a single denormalisation configuration that creates a number of denormalised documents from 3 input documents being combined. I have ran this with reference data
resulting in creation of between 1 and 12 denormalised documents per input document. This appears to scale slightly better than linearly, with a total execution time formula of
T = (0.0037 x N (number of denormalisations created per run) ) + 0.0079 seconds. 

Thus processing overhead is a steady 0.0079 seconds, and each denormalisation generated takes approximately 0.0037 seconds on top of that. Given how the library has been written, it
shouldn't matter what combinations of source documents, or number of sources, are used, this type of scaling should be expected each time. This type of test should be easy to 
conduct on your own, modern hardware.

Thus on an old early 2008 MacBook Pro you can generate 10000 denormalisations from a single input document in 37 seconds. Although I've naturally not tested this yet on my machine!

## Supported features
- Create or Update of a document generates a denormalisation (see trigger-norm-create.xqy)
- Single primary key per source in new document (not multiple primary keys yet)
- Cases where multiple existing documents of each source (type) require generation of multiple denormalisation documents, including many multiples across all sources
- Optional parent document (see config-direct-parent.xqy)
- Denormalisation from both parent and child documents (see config-direct-parent.xqy)
- Shredding single document (for ODBC view resolution) (see config-odbc-shred.xqy)
- Handles static content, text, or elements (see config-odbc-shred.xqy)
- For sources with multiple candidate documents, allow all document content to be embedded rather than always generated one denormalisation per related document per new incoming content (n * m * l number of denormalisations is the default) - use n:source/@combine with value true
- Support for foreign key relationship to same source as primary key (E.g. people related to other people)
- Support for namespaces in source and generated content (Not yet for attributes with a different namespace to their parent element)
- Optional grandparent documents (and thus any ancestor docs)
 - Working Now. NB There is a dependancy on resolving parent first - required a processing order on the XML sources themselves, determined at runtime based on dependency tree (via foreign keys)

## Upcoming / not yet 
- Check against previous media company's requirements
- Test performance on previous media company's requirements (one for 20 denorms, one for 10000 denorms)
- Composite primary keys in the document that generates the denormalisation
- Shredding one document whilst merging with another document (TODO test, should work)
- Allow referring directly to an XPath to insert content in template without parent element (so you can merge multiple elements in to same parent, or copy parent with attributes)
- Allow uri pattern to refer to any value with a range index in any source document
- Support source documents where relation matches are within a specific child element, not at the top level entity. E.g. a graph holding multiple triples, but you want to use content from just one triple as a source (ML7)
- Shred at multiple hierarchical levels in source document (E.g. for ODBC view representing a doc with three levels in a tree structure)
- Checking if generated content will be the same as content that already exists (thus reducing any other triggers firing) -> May take longer than just regenerating the content
- Being part of a pipeline in CPF
- XSLT generation of new denormalisation documents
- Embedded XQuery / document by example templating (must be in internal format only)

## Long term / management application features to do
- Use Roxy and MLJS to create an admin interface and wizard for creating and editing Denormalisation configurations interactively
- Security login as admin required
- Phases - List denormalisations & show db name used, Describe, Add sources, define relationships, Create output template, Add output settings/activate, save
- Add basic preview functionality
 - On add source phase, show list of matching docs with source definition
 - In relationship test mode, specify trigger source, then select mock incoming doc, then list and view all related docs next to each source definition, and preview sample output when ready
- Add quick edit options to relationship test mode, so you don't have to-ing and fro-ing between edit and test pages
- Add sanity checks when saving denormalisation. E.g. orphaned sources, output not referring to sources, URI pattern not referring to sources or auto, invalid XPaths used
- Add helper functions for specifying elements to match. E.g. XML preview with clickable elements/attributes
- Add indexes required preview, check exist, and auto creation capabilities on save (Without indexes present, don't allow 'enabled' to be 'true')
- Add helper 'create output template by example' feature, with drop downs for source and indexes available
- Script to install norm (Via Roxy bootstrap, deploy modules) then use REST functions to modify this for the target content and modules DBs, and deploy norm.xqy in to that target DB
- Setup content database triggers within admin app
- Install Norm automatically via REST API for target database (will require MLJS cross domain JavaScript security to work first)
- Make content/modules db setup dynamic, selectable from drop down of REST app servers (will require MLJS cross domain JavaScript security to work first)
- Allow creation of REST app server automatically based on selecting content database (will require MLJS cross domain JavaScript security to work first)
- Enable denormalisation profiling by doing 10 test runs spread 4 seconds apart
- Enable denormalisation auditing

